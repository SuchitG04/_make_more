{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a61c0b0-d595-4920-8bb1-26185de8e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63622752-ee6d-435d-ae6c-a2c84c7dfc1a",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7fd6f45-46a4-4bdc-ab76-bbb8b7c67bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"./names.txt\", \"r\").read().splitlines()\n",
    "train_words, temp_words = train_test_split(words, train_size=0.8, random_state=42)\n",
    "dev_words, test_words = train_test_split(temp_words, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeee3131-4772-446c-9858-167732546a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25626, 3203, 3204)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_words), len(dev_words), len(test_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4746f63-91c5-4ccc-a7d2-ae6df12aeb4f",
   "metadata": {},
   "source": [
    "## Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a50dcb2e-3517-48f1-a26b-51fb008cebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chars = sorted(list(set(''.join(train_words))))\n",
    "two_chars = set()\n",
    "for c1 in train_chars+[\".\"]:\n",
    "  for c2 in train_chars+[\".\"]:\n",
    "    two_chars.add(c1+c2)\n",
    "\n",
    "two_chars = sorted(list(two_chars))\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(train_chars)}\n",
    "stoi[\".\"] = 0\n",
    "stoi2 = {s:i for i,s in enumerate(two_chars)}\n",
    "itos2 = {i:s for i,s in enumerate(two_chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2db900fa-3ba7-499f-bd8e-28131f233c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_t, ys_t = [], []\n",
    "for w in train_words:\n",
    "  chs = [\".\"] + list(w) + [\".\"]\n",
    "  for ch1,ch2,ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi2[ch1+ch2]\n",
    "    ix2 = stoi[ch3]\n",
    "    xs_t.append(ix1)\n",
    "    ys_t.append(ix2)\n",
    "\n",
    "xs_t = torch.tensor(xs_t)\n",
    "ys_t = torch.tensor(ys_t)\n",
    "\n",
    "W = torch.empty(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee4befbb-7a9c-429d-8291-cf4d186ded76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=150):\n",
    "    global W\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    W = torch.randn((729, 27), generator=g, requires_grad=True)\n",
    "\n",
    "    for i in range(150):\n",
    "        # forward pass\n",
    "        logits = W[xs_t]\n",
    "        # counts = logits.exp()\n",
    "        # probs = counts / counts.sum(1, keepdim=True)\n",
    "        # loss = -probs[torch.arange(xs_t.nelement()), ys_t].log().mean() + reg_factor*(W**2).mean()\n",
    "        \n",
    "        loss = F.cross_entropy(logits, ys_t) + 0.01*(W**2).mean()\n",
    "    \n",
    "        print(f\"Epoch: {i}; Loss: {loss.item()}\")\n",
    "    \n",
    "      # backward pass\n",
    "        W.grad = None\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            W.data += -75 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d7ca3b1-dcd0-4157-aaf2-f52d954dee7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 3.7336502075195312\n",
      "Epoch: 1; Loss: 3.6212451457977295\n",
      "Epoch: 2; Loss: 3.519554376602173\n",
      "Epoch: 3; Loss: 3.4285521507263184\n",
      "Epoch: 4; Loss: 3.3480429649353027\n",
      "Epoch: 5; Loss: 3.277346134185791\n",
      "Epoch: 6; Loss: 3.215322494506836\n",
      "Epoch: 7; Loss: 3.160595417022705\n",
      "Epoch: 8; Loss: 3.1118381023406982\n",
      "Epoch: 9; Loss: 3.067963123321533\n",
      "Epoch: 10; Loss: 3.0281543731689453\n",
      "Epoch: 11; Loss: 2.991804838180542\n",
      "Epoch: 12; Loss: 2.9584555625915527\n",
      "Epoch: 13; Loss: 2.9277443885803223\n",
      "Epoch: 14; Loss: 2.8993759155273438\n",
      "Epoch: 15; Loss: 2.873103380203247\n",
      "Epoch: 16; Loss: 2.8487143516540527\n",
      "Epoch: 17; Loss: 2.8260247707366943\n",
      "Epoch: 18; Loss: 2.804870367050171\n",
      "Epoch: 19; Loss: 2.7851061820983887\n",
      "Epoch: 20; Loss: 2.7666003704071045\n",
      "Epoch: 21; Loss: 2.749232530593872\n",
      "Epoch: 22; Loss: 2.732896566390991\n",
      "Epoch: 23; Loss: 2.7174949645996094\n",
      "Epoch: 24; Loss: 2.7029411792755127\n",
      "Epoch: 25; Loss: 2.6891579627990723\n",
      "Epoch: 26; Loss: 2.676076889038086\n",
      "Epoch: 27; Loss: 2.663638114929199\n",
      "Epoch: 28; Loss: 2.651787519454956\n",
      "Epoch: 29; Loss: 2.640477418899536\n",
      "Epoch: 30; Loss: 2.6296660900115967\n",
      "Epoch: 31; Loss: 2.619316577911377\n",
      "Epoch: 32; Loss: 2.609394073486328\n",
      "Epoch: 33; Loss: 2.599869966506958\n",
      "Epoch: 34; Loss: 2.59071683883667\n",
      "Epoch: 35; Loss: 2.5819103717803955\n",
      "Epoch: 36; Loss: 2.5734291076660156\n",
      "Epoch: 37; Loss: 2.5652520656585693\n",
      "Epoch: 38; Loss: 2.5573620796203613\n",
      "Epoch: 39; Loss: 2.5497422218322754\n",
      "Epoch: 40; Loss: 2.5423777103424072\n",
      "Epoch: 41; Loss: 2.53525447845459\n",
      "Epoch: 42; Loss: 2.5283591747283936\n",
      "Epoch: 43; Loss: 2.5216808319091797\n",
      "Epoch: 44; Loss: 2.5152082443237305\n",
      "Epoch: 45; Loss: 2.5089316368103027\n",
      "Epoch: 46; Loss: 2.502840995788574\n",
      "Epoch: 47; Loss: 2.4969279766082764\n",
      "Epoch: 48; Loss: 2.491184711456299\n",
      "Epoch: 49; Loss: 2.4856033325195312\n",
      "Epoch: 50; Loss: 2.4801769256591797\n",
      "Epoch: 51; Loss: 2.474898338317871\n",
      "Epoch: 52; Loss: 2.4697623252868652\n",
      "Epoch: 53; Loss: 2.464761972427368\n",
      "Epoch: 54; Loss: 2.459892511367798\n",
      "Epoch: 55; Loss: 2.455148220062256\n",
      "Epoch: 56; Loss: 2.4505248069763184\n",
      "Epoch: 57; Loss: 2.446016788482666\n",
      "Epoch: 58; Loss: 2.441620349884033\n",
      "Epoch: 59; Loss: 2.4373316764831543\n",
      "Epoch: 60; Loss: 2.4331462383270264\n",
      "Epoch: 61; Loss: 2.4290599822998047\n",
      "Epoch: 62; Loss: 2.42507004737854\n",
      "Epoch: 63; Loss: 2.421172857284546\n",
      "Epoch: 64; Loss: 2.4173645973205566\n",
      "Epoch: 65; Loss: 2.4136431217193604\n",
      "Epoch: 66; Loss: 2.4100048542022705\n",
      "Epoch: 67; Loss: 2.406447649002075\n",
      "Epoch: 68; Loss: 2.402967929840088\n",
      "Epoch: 69; Loss: 2.3995635509490967\n",
      "Epoch: 70; Loss: 2.3962323665618896\n",
      "Epoch: 71; Loss: 2.392971992492676\n",
      "Epoch: 72; Loss: 2.389780282974243\n",
      "Epoch: 73; Loss: 2.386654853820801\n",
      "Epoch: 74; Loss: 2.3835933208465576\n",
      "Epoch: 75; Loss: 2.380594491958618\n",
      "Epoch: 76; Loss: 2.3776562213897705\n",
      "Epoch: 77; Loss: 2.3747763633728027\n",
      "Epoch: 78; Loss: 2.3719537258148193\n",
      "Epoch: 79; Loss: 2.3691864013671875\n",
      "Epoch: 80; Loss: 2.3664724826812744\n",
      "Epoch: 81; Loss: 2.3638110160827637\n",
      "Epoch: 82; Loss: 2.3612000942230225\n",
      "Epoch: 83; Loss: 2.3586387634277344\n",
      "Epoch: 84; Loss: 2.3561248779296875\n",
      "Epoch: 85; Loss: 2.3536581993103027\n",
      "Epoch: 86; Loss: 2.35123610496521\n",
      "Epoch: 87; Loss: 2.348858594894409\n",
      "Epoch: 88; Loss: 2.3465232849121094\n",
      "Epoch: 89; Loss: 2.3442301750183105\n",
      "Epoch: 90; Loss: 2.34197735786438\n",
      "Epoch: 91; Loss: 2.339764356613159\n",
      "Epoch: 92; Loss: 2.337589740753174\n",
      "Epoch: 93; Loss: 2.3354525566101074\n",
      "Epoch: 94; Loss: 2.3333520889282227\n",
      "Epoch: 95; Loss: 2.3312864303588867\n",
      "Epoch: 96; Loss: 2.3292555809020996\n",
      "Epoch: 97; Loss: 2.327258825302124\n",
      "Epoch: 98; Loss: 2.3252947330474854\n",
      "Epoch: 99; Loss: 2.323362350463867\n",
      "Epoch: 100; Loss: 2.3214614391326904\n",
      "Epoch: 101; Loss: 2.319591522216797\n",
      "Epoch: 102; Loss: 2.317750930786133\n",
      "Epoch: 103; Loss: 2.315938949584961\n",
      "Epoch: 104; Loss: 2.3141555786132812\n",
      "Epoch: 105; Loss: 2.3123996257781982\n",
      "Epoch: 106; Loss: 2.3106703758239746\n",
      "Epoch: 107; Loss: 2.308967351913452\n",
      "Epoch: 108; Loss: 2.3072900772094727\n",
      "Epoch: 109; Loss: 2.305637836456299\n",
      "Epoch: 110; Loss: 2.3040099143981934\n",
      "Epoch: 111; Loss: 2.302406072616577\n",
      "Epoch: 112; Loss: 2.3008251190185547\n",
      "Epoch: 113; Loss: 2.299267530441284\n",
      "Epoch: 114; Loss: 2.297731876373291\n",
      "Epoch: 115; Loss: 2.296217679977417\n",
      "Epoch: 116; Loss: 2.294725179672241\n",
      "Epoch: 117; Loss: 2.293252944946289\n",
      "Epoch: 118; Loss: 2.2918014526367188\n",
      "Epoch: 119; Loss: 2.290369987487793\n",
      "Epoch: 120; Loss: 2.288957357406616\n",
      "Epoch: 121; Loss: 2.2875640392303467\n",
      "Epoch: 122; Loss: 2.286189556121826\n",
      "Epoch: 123; Loss: 2.2848331928253174\n",
      "Epoch: 124; Loss: 2.283494472503662\n",
      "Epoch: 125; Loss: 2.2821733951568604\n",
      "Epoch: 126; Loss: 2.280869245529175\n",
      "Epoch: 127; Loss: 2.2795822620391846\n",
      "Epoch: 128; Loss: 2.2783114910125732\n",
      "Epoch: 129; Loss: 2.277056932449341\n",
      "Epoch: 130; Loss: 2.27581787109375\n",
      "Epoch: 131; Loss: 2.27459454536438\n",
      "Epoch: 132; Loss: 2.273386001586914\n",
      "Epoch: 133; Loss: 2.2721924781799316\n",
      "Epoch: 134; Loss: 2.2710139751434326\n",
      "Epoch: 135; Loss: 2.2698493003845215\n",
      "Epoch: 136; Loss: 2.2686991691589355\n",
      "Epoch: 137; Loss: 2.2675621509552\n",
      "Epoch: 138; Loss: 2.2664389610290527\n",
      "Epoch: 139; Loss: 2.265329360961914\n",
      "Epoch: 140; Loss: 2.264232635498047\n",
      "Epoch: 141; Loss: 2.263148307800293\n",
      "Epoch: 142; Loss: 2.2620770931243896\n",
      "Epoch: 143; Loss: 2.2610180377960205\n",
      "Epoch: 144; Loss: 2.2599706649780273\n",
      "Epoch: 145; Loss: 2.2589356899261475\n",
      "Epoch: 146; Loss: 2.2579123973846436\n",
      "Epoch: 147; Loss: 2.2569003105163574\n",
      "Epoch: 148; Loss: 2.255899667739868\n",
      "Epoch: 149; Loss: 2.2549099922180176\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4dde4d0-41c2-4f23-8f65-3abc05677ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(word_set):\n",
    "    xs_t, ys_t = [], []\n",
    "    for w in word_set:\n",
    "        chs = [\".\"] + list(w) + [\".\"]\n",
    "        for ch1,ch2,ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "            ix1 = stoi2[ch1+ch2]\n",
    "            ix2 = stoi[ch3]\n",
    "            xs_t.append(ix1)\n",
    "            ys_t.append(ix2)\n",
    "    \n",
    "    xs_t = torch.tensor(xs_t)\n",
    "    ys_t = torch.tensor(ys_t)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = W[xs_t]\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdim=True)\n",
    "    \n",
    "        nll = -probs[torch.arange(xs_t.nelement()), ys_t].log().mean()\n",
    "\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7309d180-d66f-4b6f-95ce-342c716de067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2487)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_loss(dev_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c1c93-5051-4869-9148-4c90a55e4c04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
