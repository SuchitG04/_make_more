{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a61c0b0-d595-4920-8bb1-26185de8e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63622752-ee6d-435d-ae6c-a2c84c7dfc1a",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7fd6f45-46a4-4bdc-ab76-bbb8b7c67bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"./names.txt\", \"r\").read().splitlines()\n",
    "train_words, temp_words = train_test_split(words, train_size=0.8, random_state=42)\n",
    "dev_words, test_words = train_test_split(temp_words, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eeee3131-4772-446c-9858-167732546a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25626, 3203, 3204)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_words), len(dev_words), len(test_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4746f63-91c5-4ccc-a7d2-ae6df12aeb4f",
   "metadata": {},
   "source": [
    "## Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a50dcb2e-3517-48f1-a26b-51fb008cebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_chars = sorted(list(set(''.join(train_words))))\n",
    "two_chars = set()\n",
    "for c1 in train_chars+[\".\"]:\n",
    "  for c2 in train_chars+[\".\"]:\n",
    "    two_chars.add(c1+c2)\n",
    "\n",
    "two_chars = sorted(list(two_chars))\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(train_chars)}\n",
    "stoi[\".\"] = 0\n",
    "stoi2 = {s:i for i,s in enumerate(two_chars)}\n",
    "itos2 = {i:s for i,s in enumerate(two_chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2db900fa-3ba7-499f-bd8e-28131f233c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_t, ys_t = [], []\n",
    "for w in train_words:\n",
    "  chs = [\".\"] + list(w) + [\".\"]\n",
    "  for ch1,ch2,ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "    ix1 = stoi2[ch1+ch2]\n",
    "    ix2 = stoi[ch3]\n",
    "    xs_t.append(ix1)\n",
    "    ys_t.append(ix2)\n",
    "\n",
    "xs_t = torch.tensor(xs_t)\n",
    "ys_t = torch.tensor(ys_t)\n",
    "\n",
    "W = torch.empty(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee4befbb-7a9c-429d-8291-cf4d186ded76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=150):\n",
    "    global W\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    W = torch.randn((729, 27), generator=g, requires_grad=True)\n",
    "\n",
    "    for i in range(150):\n",
    "        # forward pass\n",
    "        logits = W[xs_t]\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdim=True)\n",
    "        # loss = -probs[torch.arange(xs_t.nelement()), ys_t].log().mean() + reg_factor*(W**2).mean()\n",
    "        loss = F.cross_entropy(probs, ys_t) + 0.01*(W**2).mean()\n",
    "    \n",
    "        print(f\"Epoch: {i}; Loss: {loss.item()}\")\n",
    "    \n",
    "      # backward pass\n",
    "        W.grad = None\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            W.data += -75 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d7ca3b1-dcd0-4157-aaf2-f52d954dee7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0; Loss: 3.30454158782959\n",
      "Epoch: 1; Loss: 3.3042337894439697\n",
      "Epoch: 2; Loss: 3.303915023803711\n",
      "Epoch: 3; Loss: 3.3035848140716553\n",
      "Epoch: 4; Loss: 3.3032443523406982\n",
      "Epoch: 5; Loss: 3.3028922080993652\n",
      "Epoch: 6; Loss: 3.302529811859131\n",
      "Epoch: 7; Loss: 3.302157402038574\n",
      "Epoch: 8; Loss: 3.3017749786376953\n",
      "Epoch: 9; Loss: 3.3013832569122314\n",
      "Epoch: 10; Loss: 3.30098295211792\n",
      "Epoch: 11; Loss: 3.3005733489990234\n",
      "Epoch: 12; Loss: 3.3001549243927\n",
      "Epoch: 13; Loss: 3.29972767829895\n",
      "Epoch: 14; Loss: 3.2992913722991943\n",
      "Epoch: 15; Loss: 3.298845052719116\n",
      "Epoch: 16; Loss: 3.2983877658843994\n",
      "Epoch: 17; Loss: 3.2979187965393066\n",
      "Epoch: 18; Loss: 3.2974355220794678\n",
      "Epoch: 19; Loss: 3.2969377040863037\n",
      "Epoch: 20; Loss: 3.296422004699707\n",
      "Epoch: 21; Loss: 3.295886993408203\n",
      "Epoch: 22; Loss: 3.2953293323516846\n",
      "Epoch: 23; Loss: 3.2947463989257812\n",
      "Epoch: 24; Loss: 3.294135570526123\n",
      "Epoch: 25; Loss: 3.2934932708740234\n",
      "Epoch: 26; Loss: 3.292816638946533\n",
      "Epoch: 27; Loss: 3.2921030521392822\n",
      "Epoch: 28; Loss: 3.2913506031036377\n",
      "Epoch: 29; Loss: 3.2905571460723877\n",
      "Epoch: 30; Loss: 3.2897212505340576\n",
      "Epoch: 31; Loss: 3.288844585418701\n",
      "Epoch: 32; Loss: 3.287928342819214\n",
      "Epoch: 33; Loss: 3.286975622177124\n",
      "Epoch: 34; Loss: 3.285989999771118\n",
      "Epoch: 35; Loss: 3.284977912902832\n",
      "Epoch: 36; Loss: 3.2839460372924805\n",
      "Epoch: 37; Loss: 3.282902956008911\n",
      "Epoch: 38; Loss: 3.281858205795288\n",
      "Epoch: 39; Loss: 3.280818462371826\n",
      "Epoch: 40; Loss: 3.2797906398773193\n",
      "Epoch: 41; Loss: 3.278777837753296\n",
      "Epoch: 42; Loss: 3.2777819633483887\n",
      "Epoch: 43; Loss: 3.276801586151123\n",
      "Epoch: 44; Loss: 3.2758336067199707\n",
      "Epoch: 45; Loss: 3.274874687194824\n",
      "Epoch: 46; Loss: 3.2739219665527344\n",
      "Epoch: 47; Loss: 3.272970199584961\n",
      "Epoch: 48; Loss: 3.272017478942871\n",
      "Epoch: 49; Loss: 3.2710630893707275\n",
      "Epoch: 50; Loss: 3.270106077194214\n",
      "Epoch: 51; Loss: 3.269148111343384\n",
      "Epoch: 52; Loss: 3.268190622329712\n",
      "Epoch: 53; Loss: 3.2672383785247803\n",
      "Epoch: 54; Loss: 3.2662956714630127\n",
      "Epoch: 55; Loss: 3.265367031097412\n",
      "Epoch: 56; Loss: 3.2644574642181396\n",
      "Epoch: 57; Loss: 3.2635715007781982\n",
      "Epoch: 58; Loss: 3.26271390914917\n",
      "Epoch: 59; Loss: 3.261884927749634\n",
      "Epoch: 60; Loss: 3.2610888481140137\n",
      "Epoch: 61; Loss: 3.260324478149414\n",
      "Epoch: 62; Loss: 3.259592294692993\n",
      "Epoch: 63; Loss: 3.2588911056518555\n",
      "Epoch: 64; Loss: 3.2582192420959473\n",
      "Epoch: 65; Loss: 3.2575740814208984\n",
      "Epoch: 66; Loss: 3.2569544315338135\n",
      "Epoch: 67; Loss: 3.2563564777374268\n",
      "Epoch: 68; Loss: 3.2557780742645264\n",
      "Epoch: 69; Loss: 3.2552170753479004\n",
      "Epoch: 70; Loss: 3.254671335220337\n",
      "Epoch: 71; Loss: 3.254138231277466\n",
      "Epoch: 72; Loss: 3.2536160945892334\n",
      "Epoch: 73; Loss: 3.2531023025512695\n",
      "Epoch: 74; Loss: 3.252596616744995\n",
      "Epoch: 75; Loss: 3.2520956993103027\n",
      "Epoch: 76; Loss: 3.2516002655029297\n",
      "Epoch: 77; Loss: 3.2511074542999268\n",
      "Epoch: 78; Loss: 3.2506167888641357\n",
      "Epoch: 79; Loss: 3.250126600265503\n",
      "Epoch: 80; Loss: 3.2496373653411865\n",
      "Epoch: 81; Loss: 3.249147653579712\n",
      "Epoch: 82; Loss: 3.2486562728881836\n",
      "Epoch: 83; Loss: 3.2481632232666016\n",
      "Epoch: 84; Loss: 3.2476677894592285\n",
      "Epoch: 85; Loss: 3.247169017791748\n",
      "Epoch: 86; Loss: 3.2466676235198975\n",
      "Epoch: 87; Loss: 3.2461624145507812\n",
      "Epoch: 88; Loss: 3.2456533908843994\n",
      "Epoch: 89; Loss: 3.2451393604278564\n",
      "Epoch: 90; Loss: 3.244621515274048\n",
      "Epoch: 91; Loss: 3.24409818649292\n",
      "Epoch: 92; Loss: 3.243570566177368\n",
      "Epoch: 93; Loss: 3.2430362701416016\n",
      "Epoch: 94; Loss: 3.242496967315674\n",
      "Epoch: 95; Loss: 3.241950750350952\n",
      "Epoch: 96; Loss: 3.2413971424102783\n",
      "Epoch: 97; Loss: 3.240835189819336\n",
      "Epoch: 98; Loss: 3.240265130996704\n",
      "Epoch: 99; Loss: 3.23968505859375\n",
      "Epoch: 100; Loss: 3.239093542098999\n",
      "Epoch: 101; Loss: 3.238490104675293\n",
      "Epoch: 102; Loss: 3.2378735542297363\n",
      "Epoch: 103; Loss: 3.2372424602508545\n",
      "Epoch: 104; Loss: 3.23659610748291\n",
      "Epoch: 105; Loss: 3.235933303833008\n",
      "Epoch: 106; Loss: 3.23525333404541\n",
      "Epoch: 107; Loss: 3.234555244445801\n",
      "Epoch: 108; Loss: 3.2338380813598633\n",
      "Epoch: 109; Loss: 3.2331008911132812\n",
      "Epoch: 110; Loss: 3.2323434352874756\n",
      "Epoch: 111; Loss: 3.231563091278076\n",
      "Epoch: 112; Loss: 3.230759382247925\n",
      "Epoch: 113; Loss: 3.229931592941284\n",
      "Epoch: 114; Loss: 3.2290775775909424\n",
      "Epoch: 115; Loss: 3.228198528289795\n",
      "Epoch: 116; Loss: 3.2272958755493164\n",
      "Epoch: 117; Loss: 3.2263739109039307\n",
      "Epoch: 118; Loss: 3.2254393100738525\n",
      "Epoch: 119; Loss: 3.224501132965088\n",
      "Epoch: 120; Loss: 3.223569631576538\n",
      "Epoch: 121; Loss: 3.2226572036743164\n",
      "Epoch: 122; Loss: 3.221773862838745\n",
      "Epoch: 123; Loss: 3.2209279537200928\n",
      "Epoch: 124; Loss: 3.220126152038574\n",
      "Epoch: 125; Loss: 3.219369411468506\n",
      "Epoch: 126; Loss: 3.2186596393585205\n",
      "Epoch: 127; Loss: 3.2179946899414062\n",
      "Epoch: 128; Loss: 3.2173709869384766\n",
      "Epoch: 129; Loss: 3.216784715652466\n",
      "Epoch: 130; Loss: 3.216233015060425\n",
      "Epoch: 131; Loss: 3.2157111167907715\n",
      "Epoch: 132; Loss: 3.2152156829833984\n",
      "Epoch: 133; Loss: 3.2147433757781982\n",
      "Epoch: 134; Loss: 3.214291572570801\n",
      "Epoch: 135; Loss: 3.213858127593994\n",
      "Epoch: 136; Loss: 3.213440179824829\n",
      "Epoch: 137; Loss: 3.213036060333252\n",
      "Epoch: 138; Loss: 3.212644338607788\n",
      "Epoch: 139; Loss: 3.212263345718384\n",
      "Epoch: 140; Loss: 3.21189284324646\n",
      "Epoch: 141; Loss: 3.2115306854248047\n",
      "Epoch: 142; Loss: 3.211176633834839\n",
      "Epoch: 143; Loss: 3.2108304500579834\n",
      "Epoch: 144; Loss: 3.21049165725708\n",
      "Epoch: 145; Loss: 3.210158348083496\n",
      "Epoch: 146; Loss: 3.209831476211548\n",
      "Epoch: 147; Loss: 3.209510087966919\n",
      "Epoch: 148; Loss: 3.2091939449310303\n",
      "Epoch: 149; Loss: 3.208883047103882\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4dde4d0-41c2-4f23-8f65-3abc05677ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(word_set):\n",
    "    xs_t, ys_t = [], []\n",
    "    for w in word_set:\n",
    "        chs = [\".\"] + list(w) + [\".\"]\n",
    "        for ch1,ch2,ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "            ix1 = stoi2[ch1+ch2]\n",
    "            ix2 = stoi[ch3]\n",
    "            xs_t.append(ix1)\n",
    "            ys_t.append(ix2)\n",
    "    \n",
    "    xs_t = torch.tensor(xs_t)\n",
    "    ys_t = torch.tensor(ys_t)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = W[xs_t]\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdim=True)\n",
    "    \n",
    "        nll = -probs[torch.arange(xs_t.nelement()), ys_t].log().mean()\n",
    "\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7309d180-d66f-4b6f-95ce-342c716de067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5356)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_loss(dev_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c1c93-5051-4869-9148-4c90a55e4c04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
